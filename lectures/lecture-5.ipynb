{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Lecture 5: Priors on function spaces. Gaussian Process regression. The Karhunen–Loève expansion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Problem setting\n",
    "* We have an unknown multivariate function $f(\\mathbf{x})$ that we would like to approximate on some specified domain.<br> \n",
    "* We have a dataset $\\mathcal{D}$ of $n$ function observations, $\\mathcal{D} = {(\\mathbf{x}_i,y_i),i = 1,\\ldots,n}$.\n",
    "* Given $\\mathcal{D}$ we wish to make predictions for new inputs $\\mathbf{x}_*$ within the domain.\n",
    "\n",
    "* **To solve this problem we must make assumptions about the characteristics of $f(\\mathbf{x})$**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Two common approaches\n",
    "\n",
    "* restrict the class of functions that we consider (e.g. considering only linear functions)\n",
    "    * **Problem**: we have to decide upon the richness of the class of functions considered; $f(\\mathbf{x})$ can be not well modelled by this class, so the predictions will be poor.\n",
    "\n",
    "* the second approach is (*speaking rather loosely*) to give a prior probability to every possible function, where higher probabilities are given to functions that we consider to be more likely.\n",
    "    * <span style=\"color:red\">**Problem**: there are an uncountably infinite set of possible functions.</span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Second approach \n",
    "#### This is where the Gaussian process (GP) arise to cope with the problem mentioned above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Typically, there is some knowledge about a function of interest $f(\\mathbf{x})$ even before observing it anywhere.\n",
    "\n",
    "For example, $f(\\mathbf{x})$ cannot exceed, or be smaller than, certain values or that it is periodic or that it shows translational invariance.<br>\n",
    "Such knowledge is called as the **prior knowledge**.\n",
    "\n",
    "**Prior knowledge** may be precise (e.g., $f(\\mathbf{x})$ is twice differentiable), or it may be vague (e.g., the probability, that the periodicity is $T$, is $p(T)$). When we have a deal with vague prior knowledge, we refer to it as **prior belief**. \n",
    "\n",
    "**Prior beliefs** about $f(\\mathbf{x})$ can be modeled by a probability measure on the space of functions from $\\mathcal{F}$ to $\\mathbb{R}$. A GP is a great way to represent this probability measure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Definition of GP\n",
    "**A Gaussian process is a collection of random variables, any finite number of which have a joint Gaussian distribution (in other words GP is a generalization of a multivariate Gaussian distribution to infinite dimensions).** \n",
    "\n",
    "A GP defines a probability measure on $\\mathcal{F}$. When we say that $f(\\mathbf{x})$ is a GP, we mean that it is a random variable that is actually a function. \n",
    "\n",
    "Analytically, it can be written as\n",
    "$$\n",
    "f(\\mathbf{x}) \\sim \\mbox{GP}\\left(m(\\mathbf{x}), k(\\mathbf{x}, \\mathbf{x'}) \\right),\n",
    "$$ where  \n",
    "* $m:\\mathbb{R}^d \\rightarrow \\mathbb{R}$ is the mean function; \n",
    "* $k:\\mathbb{R}^d \\times \\mathbb{R}^d \\rightarrow \\mathbb{R}$ is the covariance function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Connection to the multivariate Gaussian distribution\n",
    "\n",
    "Let $\\mathbf{x}_{1:n}=\\{\\mathbf{x}_1,\\dots,\\mathbf{x}_n\\}$ be $n$ points in $\\mathbb{R}^d$. Let $\\mathbf{f}\\in\\mathbb{R}^n$ be the outputs of $f(\\mathbf{x})$ on each one of the elements of $\\mathbf{x}_{1:n}$,\n",
    "$$\n",
    "\\mathbf{f} =\n",
    "\\left(\n",
    "\\begin{array}{c}\n",
    "f(\\mathbf{x}_1)\\\\\n",
    "\\vdots\\\\\n",
    "f(\\mathbf{x}_n)\n",
    "\\end{array}\n",
    "\\right).\n",
    "$$\n",
    "The fact that $f(\\mathbf{x})$ is a GP with mean and covariance function $m(\\mathbf{x})$ and $k(\\mathbf{x},\\mathbf{x'})$ means that the vector of outputs $\\mathbf{f}$ at the arbitrary inputs is the following multivariate-normal: $$\n",
    "\\mathbf{f} \\sim \\mathcal{N}\\bigl(\\mathbf{m}(\\mathbf{x}_{1:n}), \\mathbf{K}(\\mathbf{x}_{1:n}, \\mathbf{x}_{1:n})\\bigr),\n",
    "$$ with mean vector: $$\n",
    "\\mathbf{m}(\\mathbf{x}_{1:n}) =\n",
    "\\left(\n",
    "\\begin{array}{c}\n",
    "m(\\mathbf{x}_1)\\\\\n",
    "\\vdots\\\\\n",
    "m(\\mathbf{x}_n)\n",
    "\\end{array}\n",
    "\\right),\n",
    "$$ and covariance matrix: $$\n",
    "\\mathbf{K}(\\mathbf{x}_{1:n},\\mathbf{x}_{1:n}) = \\left(\n",
    "\\begin{array}{ccc}\n",
    "k(\\mathbf{x}_1,\\mathbf{x}_1) & \\dots &k(\\mathbf{x}_1, \\mathbf{x}_n)\\\\\n",
    "\\vdots& \\ddots& \\vdots\\\\\n",
    "k(\\mathbf{x}_n, \\mathbf{x}_1)&  \\dots &k(\\mathbf{x}_n, \\mathbf{x}_n)\n",
    "\\end{array}\n",
    "\\right).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Now, since we have defined a GP, let us talk about how do we encode our **prior beliefs** into a GP. \n",
    "\n",
    "We do so through the **mean** and **covariance functions**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Interpretation of the mean function\n",
    "For any point $\\mathbf{x}\\in\\mathbb{R}^d$, $m(\\mathbf{x})$ is the expected value of the r.v. $f(\\mathbf{x})$:\n",
    "$$\n",
    "m(\\mathbf{x}) = \\mathbb{E}[f(\\mathbf{x})].\n",
    "$$\n",
    "The mean function can be any arbitrary function. Essentially, it tracks generic trends in the response as the input is varied.<br> \n",
    "In practice, we try and make a suitable choice for the mean function that is easy to work with. Such choices include:\n",
    "* a constant, $m(\\mathbf{x}) = c,$ where $c$ is a parameter (in many cases $c=0$).\n",
    "* linear, $m(\\mathbf{x}) = c_0 + \\sum_{i=1}^dc_ix_i,$ where $c_i, i=0,\\dots,d$ are parameters.\n",
    "* using a set of $m$ basis functions (generalized linear model), $m(\\mathbf{x}) = \\sum_{i=1}^mc_i\\phi_i(\\mathbf{x})$, where $c_i$ and $\\phi_i(\\cdot)$ are parameters and basis functions.\n",
    "* generalized polynomial chaos (gPC), using a set of $d$ polynomial basis functions upto a given degree $\\rho$, $m(\\mathbf{x}) = \\sum_{i=1}^{d}c_i\\phi_i(\\mathbf{x})$ where the basis functions $\\phi_i$ are mutually orthonormal: $$\n",
    "\\int \\phi_{i}(\\mathbf{x}) \\phi_{j}(\\mathbf{x}) dF(\\mathbf{x}) = \\delta_{ij}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Interpretation of the covariance function\n",
    "Let $\\mathbf{x}\\in\\mathbb{R}^d$. Then $k(\\mathbf{x}, \\mathbf{x})$ is the variance of the random variable $f(\\mathbf{x})$, i.e., $$\n",
    "k(\\mathbf{x},\\mathbf{x}) = \\mathbb{V}[f(\\mathbf{x})] = \\mathbb{E}\\left[\\left(f(\\mathbf{x}) - m(\\mathbf{x}) \\right)^2 \\right].\n",
    "$$ In other words, we believe that there is about $95\\%$ probability that the value of the r.v. $f(\\mathbf{x})$ fall within the interval: $$\n",
    "\\left(m(\\mathbf{x}) - 2\\sqrt{k(\\mathbf{x}, \\mathbf{x})}, m(\\mathbf{x}) + 2\\sqrt{k(\\mathbf{x},\\mathbf{x})}\\right).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Interpretation of the covariance function\n",
    "Let $\\mathbf{x},\\mathbf{x'} \\in \\mathbb{R}^d$. Then $k(\\mathbf{x}, \\mathbf{x}')$ tells us how the r.v. $f(\\mathbf{x})$ and $f(\\mathbf{x}')$ are correlated. \n",
    "\n",
    "In particular, $k(\\mathbf{x},\\mathbf{x}')$ is equal to the covariance of the r.v. $f(\\mathbf{x})$ and $f(\\mathbf{x}')$, i.e., $$\n",
    "k(\\mathbf{x}, \\mathbf{x}') = \\mathbb{E}\\left[\n",
    "\\bigl(f(\\mathbf{x}) - m(\\mathbf{x})\\bigr)\n",
    "\\bigl(f(\\mathbf{x}') - m(\\mathbf{x}')\\bigr)\n",
    "\\right].\n",
    "$$\n",
    "Essentially, a covariance function (or **covariance kernel**) defines a similarity measure on the input space. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Properties of the covariance function\n",
    "* For any $\\mathbf{x}\\in\\mathbb{R}^d$, $k(\\mathbf{x}, \\mathbf{x})\\geqslant0$. This is easly understood by the interpretation of $k(\\mathbf{x}, \\mathbf{x})$ as the variance of the random variable $f(\\mathbf{x})$.\n",
    "* For any choice of points $\\mathbf{X}\\in\\mathbb{R}^{n\\times d}$, the covariance matrix: $\\mathbf{K}(\\mathbf{X}, \\mathbf{X})$ has to be positive-semidefinite (so that the vector of outputs $\\mathbf{f}$ is indeed a multivariate normal distribution).\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Encoding prior beliefs in the covariance function\n",
    "* **Modeling regularity**. The choice of the covariance function controls the regularity properties of the functions sampled from the probability induced by the GP. For example, if the covariance kernel chosen is the squared exponential kernel, which is infinitely differentiable, then the functions sampled from the GP will also be infinitely differentiable.\n",
    "* **Modeling invariance**. If the covariance kernel is invariant w.r.t. a transformation $T$, i.e., $k(\\mathbf{x}, T\\mathbf{x}')=k(T\\mathbf{x}, \\mathbf{x}')=k(\\mathbf{x}, \\mathbf{x}')$ then samples from the GP will be invariant w.r.t. the same transformation.\n",
    "* Other possibilities include periodicity, additivity etc.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Squared exponential covariance function\n",
    "Squared exponential (SE) is widely used covariance function. Its has the form: \n",
    "$$\n",
    "k(\\mathbf{x}, \\mathbf{x}') = v\\exp\\left\\{-\\frac{1}{2}\\sum_{i=1}^d\\frac{(x_i - x_i')^2}{\\ell_i^2}\\right\\},\n",
    "$$ \n",
    "where \n",
    "* $v>0$ – signal strength. The bigger it is, the more the GP $f(\\mathbf{x})$ will vary about the mean.\n",
    "* $\\ell_i>0, i=1,\\dots,d$ – length-scale of the $i$-th input dimension of the GP. The bigger it is, the smoother the samples of $f(\\mathbf{x})$ appear along the $i$-th input dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# 1-D example\n",
    "from ipywidgets import interactive, interact, widgets\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy.spatial as SP\n",
    "\n",
    "# defining Squared Exponential Kernel and plot it\n",
    "\n",
    "def k(length_scale):\n",
    "    x = np.arange(0., 5., 0.1)\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    plt.ylim([0, 1.05])\n",
    "    plt.xlabel('$x$', fontsize=16)\n",
    "    plt.ylabel('$k(x,0)$', fontsize=16)\n",
    "    plt.plot(x, np.exp(-.5 * x**2/length_scale**2), 'b-')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "controls = {r'length_scale': widgets.FloatSlider(\n",
    "    min=0.01, max=5.0, step=0.1, value=1., continuous_update=False, description=r'$\\ell$')}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "interact(k, **controls);           "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Properties of the covariance matrix\n",
    "Let $\\mathbf{x}_{1:n}$ be an arbitrary set of input points. The covariance matrix $\\mathbf{K}\\in\\mathbb{R}^{n\\times n}$ defined by: $$\n",
    "\\mathbf{K}\\equiv\\mathbf{K}(\\mathbf{x}_{1:n}, \\mathbf{x}_{1:n}) = \\left(\n",
    "\\begin{array}{ccc}\n",
    "k(\\mathbf{x}_1,\\mathbf{x}_1) &\\dots& k(\\mathbf{x}_1, \\mathbf{x}_n)\\\\\n",
    "\\vdots&\\ddots &\\vdots\\\\\n",
    "k(\\mathbf{x}_n, \\mathbf{x}_1)&\\dots& k(\\mathbf{x}_n, \\mathbf{x}_n)\n",
    "\\end{array}\n",
    "\\right),\n",
    "$$ must be positive definite.<br> \n",
    "Mathematically, this can be expressed in two equivalent ways:\n",
    "* for all vectors $\\mathbf{v}\\in\\mathbb{R}^n$, we have: $$\n",
    "\\mathbf{v}^T \\mathbf{K}\\mathbf{v}\\geqslant 0,\n",
    "$$\n",
    "* all the eigenvalues of $\\mathbf{K}$ are positive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def plot_eigen(n, length_scale, variance):\n",
    "    np.random.seed(1)\n",
    "    X = np.random.rand(n, 1)\n",
    "    sqdist = np.sum(X**2,1).reshape(-1,1) + np.sum(X**2,1) - 2*np.dot(X, X.T)\n",
    "    K = variance*np.exp(-.5 * (1/length_scale**2) * sqdist)\n",
    "\n",
    "    eig_val = np.linalg.eigh(K)[0][::-1] # Sort'em\n",
    "\n",
    "    # Plot the eigenvalues\n",
    "    fig, ax = plt.subplots(figsize=(9,7))\n",
    "    ax.plot(np.arange(1, n+1), eig_val, 'g.', markersize=10)\n",
    "    ax.set_xlabel('$i$', fontsize=16)\n",
    "    ax.set_ylabel('$\\lambda_i$', fontsize=16)\n",
    "    plt.show()\n",
    "\n",
    "controls = {r'n' : widgets.IntSlider(min=1,max=100,step=1,value=40, continuous_update=True,description=r'$N$ of points'),\n",
    "            r'length_scale' : widgets.FloatSlider(min=0.1,max=1.0,step=0.05,value=0.01, continuous_update=True,description=r'$\\ell$'),\n",
    "            r'variance' : widgets.FloatSlider(min=0.01,max=5.0,step=0.05,value=1., continuous_update=True,description=r'$\\nu$')}    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "interact(plot_eigen, **controls);    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### The choice of covariance function\n",
    "\n",
    "So far we considered \n",
    "$$\n",
    "k(\\mathbf{x}, \\mathbf{x}') = v\\exp\\left\\{-\\frac{1}{2}\\sum_{i=1}^d\\frac{(x_i - x_i')^2}{\\ell_i^2}\\right\\}.\n",
    "$$\n",
    "\n",
    "**The question:** how can we modify this function and what will be the results?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### SE with scaled Eucledian distance\n",
    "\n",
    "\n",
    "$$\n",
    "k(\\mathbf{x}, \\mathbf{x}') = v\\exp\\left\\{-\\frac{1}{2 \\ell}\\sum_{i=1}^d(x_i - x_i')^2\\right\\}.\n",
    "$$\n",
    "\n",
    "<img src='figures/simple.png' style=\"float: left; width: 50%; margin-right: 1%;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### SE with weighted Eucledian distance\n",
    "\n",
    "$$\n",
    "k(\\mathbf{x}, \\mathbf{x}') = v\\exp\\left\\{-\\frac{1}{2}\\sum_{i=1}^d\\frac{(x_i - x_i')^2}{\\ell_i^2}\\right\\}.\n",
    "$$\n",
    "\n",
    "<img src='figures/weighted.png' style=\"float: left; width: 50%; margin-right: 1%;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### SE with Mahalonobis distance\n",
    "\n",
    "$$\n",
    "k(\\mathbf{x}, \\mathbf{x}') = v\\exp\\left\\{-\\frac{1}{2}\\sum_{i=1}^d (\\mathbf{x} - \\mathbf{x}')^T A (\\mathbf{x} - \\mathbf{x}')\\right\\}.\n",
    "$$\n",
    "for some positive definite matrix $A \\in \\mathbb{R}^{d \\times d}$.\n",
    "\n",
    "<img src='figures/Mahalonobis.png' style=\"float: left; width: 50%; margin-right: 1%;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### GP sampling\n",
    "\n",
    "* Take a finite number of inputs: $\\mathbf{x}_{1:n}=\\{\\mathbf{x}_1,\\dots,\\mathbf{x}_n\\}, \\mathbf{x}_i\\in\\mathbb{R}^d$.\n",
    "Consider the model output on them: $\\mathbf{f} = \\{f(\\mathbf{x}_1),\\ldots,f(\\mathbf{x}_n)\\}, \\mathbf{f}\\in\\mathbb{R}^n$.\n",
    "* Our *prior belief* is that they are distributed according to: $\\mathbf{f}\\sim\\mathcal{N}(\\mathbf{m},\\mathbf{K})$. \n",
    "* In order to sample from $\\mathbf{f}\\sim\\mathcal{N}(\\mathbf{m},\\mathbf{K})$:\n",
    "    * Take the Cholesky decomposition of $\\mathbf{K}$: $\\mathbf{K} = \\mathbf{L}\\mathbf{L}^T$.\n",
    "    * Sample a $n$ independent standard normal variables: $\\mathbf{z}\\sim\\mathcal{N}(\\mathbf{0}_n,\\mathbf{I}_n)$.\n",
    "    * Set $\\mathbf{f} = \\mathbf{m} + \\mathbf{L}\\mathbf{z}$.\n",
    "    \n",
    "One can see that\n",
    "$$\n",
    "\\mathbb E[\\mathbf f]=\\mathbf m,\n",
    "$$\n",
    "and\n",
    "$$\n",
    "\\text{Var}[f_i,f_j]=\\mathbb E\\left[(f_i-m_i)(f_j-m_j)\\right]=\n",
    "\\mathbb E\\left[\\left(\\sum_{m=0}^nl_{im}z_m\\right)\\left(\\sum_{r=0}^nl_{jr}z_r\\right)\\right]=\n",
    "\\mathbb E\\left[\\sum_{m=0}^nl_{im}z_ml_{jm}z_m\\right]=k_{ij},\n",
    "$$\n",
    "where $l_{ij}$ and $k_{ij}$ are elements of matrices $\\mathbf{L}$ and $\\mathbf{K}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def GP_sampling(N_samples, length_scale, n_points):\n",
    "    np.random.seed(1)\n",
    "\n",
    "    # Define the kernel\n",
    "    def kernel(a, b):\n",
    "        sqdist = SP.distance.cdist(a, b, 'sqeuclidean')\n",
    "        return np.exp(-.5 * sqdist/(length_scale**2))\n",
    "\n",
    "    # points we're going to make predictions at.\n",
    "    Xtest = np.linspace(-5, 5, n_points)[:, None]\n",
    "\n",
    "    # compute the variance at our test points.\n",
    "    K_ = kernel(Xtest, Xtest)\n",
    "    # draw samples from the prior at our test points.\n",
    "    L = np.linalg.cholesky(K_ + 1e-6*np.eye(n_points)) # a little bit regularization\n",
    "    f_prior = np.dot(L, np.random.normal(size=(n_points, N_samples)))\n",
    "    plt.figure(figsize=(9, 7))\n",
    "    plt.plot(Xtest, f_prior)\n",
    "    plt.title('Samples from the GP prior')\n",
    "    plt.xlabel('$x$', fontsize=16)\n",
    "    plt.ylabel('$f(x)$', fontsize=16)\n",
    "    plt.axis([-5, 5, -3, 3])\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "controls = {r'N_samples': widgets.IntSlider(min=1, max=30, step=1, value=10, continuous_update=True, description=r'$N$ of samples'),\n",
    "            r'length_scale': widgets.FloatSlider(min=0.1, max=2.0, step=0.05, value=0.7, continuous_update=True, description=r'$\\ell$'),\n",
    "            r'n_points': widgets.IntSlider(min=1, max=100, step=1, value=50, continuous_update=True, description=r'$N$ of points')}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "interact(GP_sampling, **controls);   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### GP regression (NO noise)\n",
    "* Let's assume that we have observed our function of interest:\n",
    "$$\n",
    "\\mathbf{X} = \\{x_1,\\ldots,x_n\\},\\\\\n",
    "\\mathbf{f} = \\{f(x_1),\\ldots,f(x_n)\\}.\n",
    "$$\n",
    "* We want to make predictions at any arbitrary set of test inputs:\n",
    "$$\n",
    "\\mathbf{X}^* = \\{x_1^*,\\ldots,x_N^*\\},\\\\\n",
    "\\mathbf{f}^* = \\{f(x_1^*),\\ldots,f(x_N^*)\\}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### GP regression (NO noise)\n",
    "* Since we have assumed that:\n",
    "$$\n",
    "f(\\mathbf{x})\\sim \\text{GP}\\left(m(\\mathbf{x}),k(\\mathbf{x},\\mathbf{x'})\\right),\n",
    "$$\n",
    "* Then according to definition:\n",
    "$$\n",
    "\\left(\n",
    "\\begin{array}{c}\n",
    "\\mathbf{f}^{\\phantom *}\\\\\n",
    "\\mathbf{f}^*\n",
    "\\end{array}\n",
    "\\right) \n",
    "\\sim\n",
    "\\mathcal{N}\n",
    "\\left(\n",
    "\\left(\n",
    "\\begin{array}{c}\n",
    "\\mathbf{m}^{\\phantom *}\\\\\n",
    "\\mathbf{m}^*\n",
    "\\end{array}\n",
    "\\right),\n",
    "\\left(\n",
    "\\begin{array}{c}\n",
    "&\\mathbf{K}(\\mathbf{X},\\mathbf{X})   &\\mathbf{K}(\\mathbf{X},\\mathbf{X}^*) \\\\\n",
    "&\\mathbf{K}(\\mathbf{X}^*,\\mathbf{X}) &\\mathbf{K}(\\mathbf{X}^*,\\mathbf{X}^*)\n",
    "\\end{array}\n",
    "\\right)\n",
    "\\right),\n",
    "$$\n",
    "\n",
    "where $\\mathbf{m}$ and $\\mathbf{m}^*$ are mean on observations and on test input accordingly,<br>\n",
    "$\\mathbf{K}(\\mathbf{X},\\mathbf{X})$ – covariance matrix of observations,<br>\n",
    "$\\mathbf{K}(\\mathbf{X}^*,\\mathbf{X})$ – cross-covariance matrix (test inputs and observations),<br>\n",
    "$\\mathbf{K}(\\mathbf{X}^*,\\mathbf{X}^*)$ – covariance matrix of test inputs.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### GP regression (NO noise)\n",
    "* Using the Bayes rule, we can obtain parameters of conditional distribution (see the proof in [Ch. 2.3 Bishop (2006)](http://users.isr.ist.utl.pt/~wurmd/Livros/school/Bishop%20-%20Pattern%20Recognition%20And%20Machine%20Learning%20-%20Springer%20%202006.pdf)): \n",
    "$$\n",
    "p(\\mathbf{f}^*~|~\\mathbf{X}^*,\\mathbf{X},\\mathbf{f})\\sim\\mathcal{N}(\\tilde{\\mathbf{m}},\\tilde{\\mathbf{K}}),\\\\\n",
    "\\tilde{\\mathbf{m}} = \\mathbf{m}^* + \\mathbf{K}(\\mathbf{X}^*,\\mathbf{X})\\mathbf{K}^{-1}(\\mathbf{f}-\\mathbf{m}),\\\\\n",
    "\\tilde{\\mathbf{K}} = \\mathbf{K}^* - \\mathbf{K}(\\mathbf{X}^*,\\mathbf{X})\\mathbf{K}^{-1}\\mathbf{K}(\\mathbf{X},\\mathbf{X}^*).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### GP regression (NO noise)\n",
    "* Since the choice of test points was arbitrary, the procedure actually defines a posterior Gaussian process: \n",
    "$$\n",
    "p(f(\\mathbf{x})~|~\\mathbf{X},\\mathbf{f})= \\text{GP}\\left(\\tilde{m}(\\mathbf{x}),\\tilde{k}(\\mathbf{x},\\mathbf{x}') \\right),\\\\\n",
    "\\tilde{m}(\\mathbf{x}) = m(\\mathbf{x}) + \\mathbf{K}(\\mathbf{x},\\mathbf{X})\\mathbf{K}^{-1}(\\mathbf{f}-\\mathbf{m}),\\\\\n",
    "\\tilde{k}(\\mathbf{x},\\mathbf{x}') = k(\\mathbf{x},\\mathbf{x}') - \\mathbf{K}(\\mathbf{x},\\mathbf{X})\\mathbf{K}^{-1}\\mathbf{K}(\\mathbf{X},\\mathbf{x}').\n",
    "$$\n",
    "* This encodes our beliefs about the model output after seeing the data.\n",
    "\n",
    "* Looking at just one point, we get the point predictive distribution.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### GP regression (WITH noise)\n",
    "* Our test inputs are noisy: $\\hat{f}(\\mathbf{x}) = f(\\mathbf{x}) + \\mathcal{N}(0,\\sigma^2)$\n",
    "* So, the posterior GP changes to:\n",
    "$$\n",
    "p(f(\\mathbf{x})~|~\\mathbf{X},\\mathbf{f}, \\sigma^2)= \\text{GP}\\left(\\tilde{m}(\\mathbf{x}),\\tilde{k}(\\mathbf{x},\\mathbf{x}') \\right),\\\\\n",
    "\\tilde{m}(\\mathbf{x}) = m(\\mathbf{x}) + \\mathbf{K}(\\mathbf{x},\\mathbf{X})(\\mathbf{K}\\color{red}{+\\sigma^2\\mathbf{I}_N})^{-1}(\\mathbf{f}-\\mathbf{m}),\\\\\n",
    "\\tilde{k}(\\mathbf{x},\\mathbf{x}') = k(\\mathbf{x},\\mathbf{x}') - \\mathbf{K}(\\mathbf{x},\\mathbf{X})(\\mathbf{K}\\color{red}{+\\sigma^2\\mathbf{I}_N})^{-1}\\mathbf{K}(\\mathbf{X},\\mathbf{x}').\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from ipywidgets import interactive\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def GP(length_scale, Test, Training, sigma):\n",
    "    np.random.seed(100)\n",
    "\n",
    "    \"\"\" This is code for simple GP regression. It assumes a zero mean GP Prior \"\"\"\n",
    "\n",
    "    # This is the true unknown function we are trying to approximate\n",
    "    def f(x): return np.sin(0.9*x.flatten())\n",
    "\n",
    "    # Define the kernel\n",
    "    def kernel(a, b):\n",
    "        sqdist = SP.distance.cdist(a, b, 'sqeuclidean')\n",
    "        return np.exp(-.5 * sqdist/(length_scale**2))\n",
    "\n",
    "    N = Training    # number of training points.\n",
    "    n = Test        # number of test points.\n",
    "    s = sigma       # noise variance.\n",
    "\n",
    "    # Sample some input points and noisy versions of the function evaluated at\n",
    "    # these points.\n",
    "    X = np.random.uniform(-5, 5, size=(N, 1))\n",
    "    y = f(X) + s*np.random.randn(N)\n",
    "\n",
    "    K = kernel(X, X)\n",
    "    L = np.linalg.cholesky(K + s*np.eye(N))\n",
    "\n",
    "    # points we're going to make predictions at.\n",
    "    Xtest = np.linspace(-5, 5, n)[:, None]\n",
    "\n",
    "    # compute the mean at our test points.\n",
    "    Lk = np.linalg.solve(L, kernel(X, Xtest))\n",
    "    mu = np.dot(Lk.T, np.linalg.solve(L, y))\n",
    "\n",
    "    # compute the variance at our test points.\n",
    "    K_ = kernel(Xtest, Xtest)\n",
    "    s2 = np.diag(K_) - np.sum(Lk**2, axis=0)\n",
    "    s  = np.sqrt(s2)\n",
    "\n",
    "    # PLOTS:\n",
    "    plt.figure(figsize=(9, 7))\n",
    "    plt.clf()\n",
    "    plt.plot(X, y, 'r+', ms=18, label=\"Training points\")\n",
    "    plt.plot(Xtest, f(Xtest), 'b-', label=\"Function\")\n",
    "    plt.gca().fill_between(Xtest.flat, mu-s, mu+s,\n",
    "                           color=\"#dddddd\", label=\"Confidence interval\")\n",
    "    plt.plot(Xtest, mu, 'r--', lw=2, label=\"Approximation\")\n",
    "    plt.title(r'Mean prediction plus-minus one s.d.')\n",
    "    plt.xlabel('$x$', fontsize=16)\n",
    "    plt.ylabel('$f(x)$', fontsize=16)\n",
    "    plt.axis([-5, 5, -3, 3])\n",
    "    plt.legend()\n",
    "    print(\"Error (inf. norm) = \", np.linalg.norm(f(Xtest)-mu, ord=np.inf)/np.linalg.norm(f(Xtest), ord=np.inf))\n",
    "    plt.show()\n",
    "controls = {r'sigma': widgets.FloatSlider(min=5e-4, max=5e-1, step=1e-3, value=1e-3, continuous_update=True, description=r'$\\sigma$'),\n",
    "            r'length_scale': widgets.FloatSlider(min=0.1, max=2.0, step=0.05, value=0.7, continuous_update=True, description=r'$\\ell$'),\n",
    "            r'Training': widgets.IntSlider(min=1, max=50, step=1, value=10, continuous_update=True, description=r'$N$ of $f$ evals'),\n",
    "            r'Test': widgets.IntSlider(min=1, max=100, step=1, value=50, continuous_update=True, description=r'$N$ of GP samples')}    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "interact(GP, **controls);  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "##  Karhunen-Loève Expansion\n",
    "\n",
    "* Above, we have to sample as many independent random variables as there are points at which we want to know the function values.\n",
    "* But what to do in the case when the function $f$ is a parameter of a PDE in the three-dimensional case, which we solve by the finite-difference scheme? Even if you take 100 points for each axis, this gives the <font color='red'>dimension</font> of inputs equal to <font color='red'>$10^6$</font>.\n",
    "* Can we significantly reduce the dimensions of the inputs, albeit with a small loss of accuracy?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Consider a random field $f(x)$, $x\\in\\mathcal X\\subset\\mathbb R^d$.\n",
    "Let us know the mean $\\mu(x)$ and the covariance function $k(x,y)=\\text{cov}\\bigl(f(x),\\,f(y)\\bigr)$ of the field $f$.\n",
    "Then Karhunen-Loève Expansion of $f$ is\n",
    "$$\n",
    "f(x,\\xi)=\\mu(x)+\\sum_{i=1}^\\infty\n",
    "\\sqrt{\\lambda_i}\\phi_i(x)\\eta_i(\\xi)\n",
    "$$\n",
    "where $\\{\\phi_i(x)\\}$ are the orthogonal eigenfunctions and $\\lambda_i$ are the corresponding eigenvalues of the eigenvalue problem\n",
    "$$\n",
    "\\int_{\\mathcal X}k(x,y)\\phi_i(y)\\,dy=\\lambda_i\\phi_i(x),\n",
    "$$\n",
    "and $\\eta_i(\\xi)$ are mutually uncorrelated random variables with zero mean\n",
    "and standard deviation equal to one\n",
    "$$\n",
    "\\mathbb E[\\eta_i]=0,\\qquad\n",
    "\\mathbb E[\\eta_i\\eta_j]=\\delta_{ij}.\n",
    "$$\n",
    "\n",
    "R.v. $\\{\\eta_i\\}$ can be defined as follows\n",
    "$$\n",
    "\\eta_i(\\xi)=\n",
    "\\frac1{\\sqrt{\\lambda_i}}\\int_{\\mathcal X}\\bigl(f(x)-\\mu(x)\\bigr)\\phi_i(x)\\,dx.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "It remains an open question under what conditions KL expansion exists.\n",
    "\n",
    "If we formally take the mathematical expectation\n",
    "$$\n",
    "\\mathbb E[f(x)]=\n",
    "\\mathbb E\\left[\\mu(x)+\\sum_{i=1}^\\infty\n",
    "\\sqrt{\\lambda_i}\\phi_i(x)\\eta_i(\\xi)\\right]=\n",
    "\\mu(x).\n",
    "$$\n",
    "\n",
    "If we formally calculate the covariance\n",
    "$$\n",
    "\\text{Cov}\\bigl(f(x),f(y)\\bigr)=\n",
    "\\mathbb E\\left[ \n",
    "\\left(\\sum_{i=1}^\\infty\n",
    "\\sqrt{\\lambda_i}\\phi_i(x)\\eta_i(\\xi)\\right)\n",
    "\\left(\\sum_{i=1}^\\infty\n",
    "\\sqrt{\\lambda_i}\\phi_i(y)\\eta_i(\\xi)\\right)\n",
    "\\right]=\\sum_{i=1}^\\infty\\lambda_i\\phi_i(x)\\phi_i(y).\n",
    "$$\n",
    "Thus, the essence of the KL expansion is reduced to the possibility of representing covariance in the form of this decomposition.\n",
    "This possibility is given us by the [Mercer's theorem](https://en.wikipedia.org/wiki/Mercer%27s_theorem) which states that if covariance function is continuous, then the convergence of this series is absolute and uniform, and all $\\lambda_i$ are non-negative.\n",
    "\n",
    "But below we will show an example where the KL expansion does not exist."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We cut off the KL series by some number $M$ of terms and this is the point, where the dimension decreases\n",
    "$$\n",
    "f(x)\\approx f_M(x)=\n",
    "\\mu(x)+\\sum_{i=1}^M\n",
    "\\sqrt{\\lambda_i}\\phi_i(x)\\eta_i(\\xi)\n",
    "$$\n",
    "\n",
    "The average variance of the error of truncation is given by the following formula\n",
    "$$\n",
    "\\varepsilon_M=\n",
    "\\int_{\\mathcal X}\n",
    "\\mathbb E[(f(x)-f_M(x))^2]\\,dx=\n",
    "\\sum_{i=M+1}^\\infty\n",
    "\\lambda_i.\n",
    "$$\n",
    "\n",
    "Thus, we can achieve the necessary accuracy by taking a certain number of terms of the expansion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Choice of $\\eta$\n",
    "### Gaussian Processes\n",
    "\n",
    "In the case when $f$ is a Gaussian process, we can select i.i.d r.v. $\\eta$ random variables, on the basis of which we model the field at all points, as an independent normal r.v. $\\eta_i(\\xi)=\\xi_i$,\n",
    "$$\n",
    "\\xi_i\\sim\\mathcal N(0,1),\\qquad\n",
    "\\mathbb E[\\xi_i\\xi_j]=\\delta_{ij}.\n",
    "$$\n",
    "For Gaussian random variables, uncorrelation and independence are equivalent,\n",
    "and linear combinations of Gaussian random variables remain Gaussian-distributed.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Non-Gaussian Processes\n",
    "\n",
    "The main problem of parametrization of non-Gaussian distributions is that uncorrelation of the random variables $\\eta_i$ does not imply independence (can you give a simple example illustrating this?).\n",
    "Hence the Karhunen-Loève expansion does not provide a way of parameterization with independent variables.\n",
    "\n",
    "But in practice, one often still uses KL expansion for an input process and then further assumes that the $\\eta_i$ are independent. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Finding eigenfunctions and eigenvalues\n",
    "In most cases, we can not analytically solve the eigenfunction equation\n",
    "$$\n",
    "\\int_{\\mathcal X}k(x,y)\\phi_i(y)\\,dy=\\lambda_i\\phi_i(x),\n",
    "$$\n",
    "but we can replace the integral by quadrature\n",
    "$$\n",
    "\\int_{\\mathcal X}k(x,y)\\phi_i(y)\\,dy\\approx\n",
    "\\sum_{i=1}^nw_ik(x,y_i)\\phi_i(y_i).\n",
    "$$\n",
    "Assume that the equation holds at the same quadrature points\n",
    "$$\n",
    "\\sum_{i=1}^nw_ik(y_j,y_i)\\phi_i(y_i)=\\phi_i(y_j),\\qquad1\\leq j\\leq n.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "So, we reduce the integral equation to the matrix eigenvalue problem\n",
    "$$\n",
    "\\mathbf K\\,\\text{diag}(w_1,\\,w_2,\\,\\ldots,\\,w_n)\\mathbf v_i=\\lambda_i\\mathbf v_i\n",
    "$$\n",
    "Then we can build an approximation to eigenfunctions\n",
    "$$\n",
    "\\phi_i(x)\n",
    "=\\frac1{\\lambda_i}\\sum_{j=1}^nw_jk(x,x_j)\\phi_i(x_j)\n",
    "=\\frac1{\\lambda_i}\\sum_{j=1}^nw_jk(x,x_j)v_{ij}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Examples\n",
    "\n",
    "In the examples below we will consider only a Gaussian processes.\n",
    "A different covariation functions will be used and we take a different number of i.i.d. random variables $\\xi_i$ for modeling the process. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Example: Exponential covariance function\n",
    "\n",
    "Let $k(x,y)=\\exp\\bigl(-\\left|x-y\\right|/a\\bigr)$\n",
    "where $a>0$ is the correlation length, and let $\\mathcal X=[-b,b]$.\n",
    "Then the eigenvalue problem \n",
    "$$\n",
    "\\int_{-b}^b\\exp\\bigl(-\\left|x-y\\right|/a\\bigr)\\phi_i(y)\\,dy=\\lambda_i\\phi_i(x)\n",
    "$$\n",
    "can be solved analytically.\n",
    "\n",
    "The eigenvalues are\n",
    "$$\n",
    "\\lambda_i=\\left\\{\n",
    "\\begin{align}\n",
    "\\frac{2a}{1+a^2w_k^2}&, &\\text{ if } i&=2k,\\\\\n",
    "\\frac{2a}{1+a^2v_k^2}&, &\\text{ if } i&=2k+1,\n",
    "\\end{align}\n",
    "\\right.\n",
    "$$\n",
    "and the corresponding eigenfunctions are\n",
    "$$\n",
    "\\phi_i(x)=\\left\\{\n",
    "\\begin{align}\n",
    "\\sin(w_kx)&\\Big/\\sqrt{b-\\frac{\\sin(2w_kb)}{2w_k}}, &\\text{ if } i&=2k,\\\\\n",
    "\\cos(v_kx)&\\Big/\\sqrt{b+\\frac{\\sin(2v_kb)}{2v_k}}, &\\text{ if } i&=2k+1,\n",
    "\\end{align}\n",
    "\\right.\n",
    "$$\n",
    "Here $w_k>0$ and $v_k>0$ are the solutions of the transcendental equations\n",
    "$$\n",
    "\\left\\{\n",
    "\\begin{align}\n",
    "aw+\\tan(wb)&=0, &\\text{ if } i&=2k,\\\\\n",
    "1-av\\tan(vb)&=0, &\\text{ if } i&=2k+1.\n",
    "\\end{align}\n",
    "\\right.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from numba import jit\n",
    "from scipy.optimize import fsolve\n",
    "from ipywidgets import interact, widgets\n",
    "\n",
    "b = 1.0\n",
    "\n",
    "\n",
    "def calc_lambda(a=1.0):\n",
    "\n",
    "    def eq_even(x, i=0):\n",
    "        return np.arctan(a*x) + (x-np.pi*i)*b\n",
    "\n",
    "    def eq_odd(x, i=0):\n",
    "        if np.abs(a*x) < 1e-18:\n",
    "            return 1e18\n",
    "        return np.arctan(1/(a*x)) - (x-np.pi*i)*b\n",
    "\n",
    "    num_l = 10\n",
    "    wi = np.array([fsolve(eq_even, 1, args=(i+1), factor=0.1, maxfev=1000)\n",
    "                   for i in range(num_l)]).flatten()\n",
    "    vi = np.array([fsolve(eq_odd,  1, args=(i))\n",
    "                   for i in range(num_l)]).flatten()\n",
    "\n",
    "    lammb = np.empty(2*num_l)\n",
    "    lammb[::2] = 2*a/(1.0 + (a*vi)**2)\n",
    "    lammb[1::2] = 2*a/(1.0 + (a*wi)**2)\n",
    "\n",
    "    check_fsolve = False\n",
    "    if check_fsolve:\n",
    "        print(wi, vi)\n",
    "        xp = np.linspace(0, 30, 1000)\n",
    "        plt.figure()\n",
    "        plt.plot(xp, -a*xp, xp, np.tan(xp*b), wi, -a*wi, 'o')\n",
    "        plt.ylim([-a*30, 1])\n",
    "        plt.figure()\n",
    "        plt.plot(xp, np.ones_like(xp), xp, a*xp *\n",
    "                 np.tan(xp*b), vi, np.ones_like(vi), 'o')\n",
    "        plt.ylim([-1, 2])\n",
    "\n",
    "    return lammb, wi, vi\n",
    "\n",
    "\n",
    "# calc_lambda(a=0.01)\n",
    "\n",
    "\n",
    "def phi(i, x, wi, vi):\n",
    "    idx, even_odd = divmod(i, 2)\n",
    "    try:\n",
    "        if even_odd == 1:\n",
    "            return np.sin(wi[idx]*x)/np.sqrt(b - np.sin(2*wi[idx]*b)/(2*wi[idx]))\n",
    "        else:\n",
    "            return np.cos(vi[idx]*x)/np.sqrt(b + np.sin(2*vi[idx]*b)/(2*vi[idx]))\n",
    "    except:\n",
    "        return np.NAN\n",
    "\n",
    "\n",
    "def plot_phi(a=1.0):\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    lammb, wi, vi = calc_lambda(a=a)\n",
    "    xp = np.linspace(-b, b, 2**10)\n",
    "    for i in range(4):\n",
    "        plt.plot(xp, phi(i, xp, wi, vi), label='i={}'.format(i+1))\n",
    "    plt.legend(loc='center right')\n",
    "    plt.title(r'Eigenfunctions $\\phi_i$')\n",
    "    plt.xlabel('$x$')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "plot_phi(a=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,7))\n",
    "for a in [10., 1., 0.1, 0.01]:\n",
    "    lammb = calc_lambda(a)[0]\n",
    "    plt.plot(range(1, len(lammb)+1), lammb, '-o', label='a={}'.format(a))\n",
    "    plt.title(r'$\\lambda_i$ with different $a$');\n",
    "    plt.yscale('log')\n",
    "    plt.xlabel('$i$')\n",
    "    plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "It can be seen from the plot that the longer the correlation length, the faster the eigenvalues decrease, and therefore the series converges faster.\n",
    "\n",
    "Also, we can see that even \n",
    "in the case when the general expressions for the eigenfunctions and eigenvalues are known, it is not so easy to calculate them. In this example, we have to solve transcendental equations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Example: Uncorrelated  process\n",
    "\n",
    "Consider the degenerate case when the correlation length is zero, i.e. $k(x,y)=\\delta(x,y)$.\n",
    "In this case, any orthogonal functions can be the eigenfunctions with constant eigenvalues $\\lambda_i=1$.\n",
    "So, the the series of eigenvalues does not converge and we can not model such a process by KL expansion. \n",
    "\n",
    "It is easy to understand why this is so: since the process values at all points are completely independent, we must simulate a continuum of values, and we can do this neither with  a finite nor with a countable number of i.i.d."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Example: Fully correlated process\n",
    "\n",
    "We consider another degenerate case, when $k(x,y)=1$ which means that the correlation length is infinity.\n",
    "In this trivial case, the process depends only on one random variable. Only one eigenvalue is not zero, and the corresponding eigenfunction is a constant one.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Expansion Optimal Linear Estimation (EOLE)\n",
    "\n",
    "As we mentioned before,\n",
    "in practice we find the eigenvalues and eigenvectors of the matrix constructed on the basis of a function in some finite set of points.\n",
    "\n",
    "The idea of Expansion Optimal Linear Estimation is based on reduction of the variance of the error of field approximation by the expansion (see the [paper](https://www.researchgate.net/publication/275188753_Optimal_Discretization_of_Random_Fields)).\n",
    "\n",
    "Consider a matrix $C_{x}$ with elements equal to the correlations of the corresponding points \n",
    "$$\n",
    "c_{ij}=C(x_i,x_j),\\qquad C(x,y)=\\frac{k(x,y)}{\\sigma(x)\\sigma(y)},\\qquad \\sigma(x)=\\sqrt{k(x,x)}\n",
    "$$\n",
    "for a set of points  $\\{x_i\\}$, $x_i\\in\\mathcal X$.\n",
    "Note, that $C_{x}$ is symmetric positive semi-definite real matrix.\n",
    "Let  $\\{\\lambda_i\\}$ and $\\{\\phi_i\\}$ are its (non-negative) eigenvalues and eigenvectors, respectively.\n",
    "We assume, that $\\{\\lambda_i\\}$ are sorted in descending order.\n",
    "Then, the following approximation $g$ of the field $f$ is considered\n",
    "$$\n",
    "g(x,\\eta)=\\sum_{i=1}^M\\frac{\\eta_i}{\\sqrt{\\lambda_i}}\\phi_i^T{\\bf C}_{x\\eta}.\n",
    "$$\n",
    "where ${\\bf C}_{x\\eta}$ is the vector with elements ${\\bf C}^{(k)}_{x\\eta}=C(x,\\eta_k)$.\n",
    "The i.i.d. random variables $\\{\\eta_i\\}$ are determines by the distribution of $f$, typically they are normal.\n",
    "\n",
    "The variance of the error of EOLE method is given by\n",
    "$$\n",
    "\\text{Var}[f(x)-g(x)]=\n",
    "\\sigma^2(x)-\\sum_{i=1}^M\\frac1{\\lambda_i}\\left(\\phi_i^TC_{x\\eta}\\right)^2.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Consider an **example** of 2D field on $[-1,\\,1]^2$ with the following correlation function\n",
    "$$\n",
    "  C(x,y)=\\exp\\left(-\\frac{|x-y|^2}{\\ell^2}\\right).\n",
    "$$\n",
    "\n",
    "We can visualize the eigenvectors $\\phi_n(x,y)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def Gen_uniform_pnts(range_O=[-1, 1], num_1axis=15):\n",
    "    \"\"\"\n",
    "    Returns uniform 2D mesh of points\n",
    "    \"\"\"\n",
    "    pnts_x = np.linspace(range_O[0], range_O[1], num_1axis)\n",
    "    xv, yv = np.meshgrid(pnts_x, pnts_x)\n",
    "    pnts = np.vstack((np.ravel(i) for i in [xv, yv])).T\n",
    "    return pnts\n",
    "\n",
    "\n",
    "class EOLE(object):\n",
    "    def __init__(self, corr_func=None, M=10, pnts=None):\n",
    "        self.corr_func = EOLE.rho if corr_func is None else corr_func\n",
    "        self.pnts = Gen_uniform_pnts() if pnts is None else pnts\n",
    "        self.M = M\n",
    "        self.stored_vals = {}\n",
    "\n",
    "        Cm = self.Cmat()\n",
    "        l, theta_r = np.linalg.eig(Cm)\n",
    "        theta = theta_r.T  # due to strange output of eig\n",
    "        l = np.real(l)\n",
    "        idx = np.argsort(l)[::-1]  # The biggest first\n",
    "        self.l = l[idx]\n",
    "        self.theta = np.real(theta[idx])\n",
    "\n",
    "    @jit\n",
    "    def __call__(self, xi, z=None):\n",
    "        \"\"\"\n",
    "        INPUTS:\n",
    "        z -- point of field\n",
    "        xi -- parameters (r.v.)\n",
    "        Uses:\n",
    "        pnts -- points in expantion\n",
    "        l -- lambda, eigenvals\n",
    "        theta -- eignvectors\n",
    "        M -- number of terms (truncation)\n",
    "        \"\"\"\n",
    "        if z is None:\n",
    "            z = xi\n",
    "            xi = self.xi\n",
    "\n",
    "        if z.ndim == 1:\n",
    "            return np.dot(xi[:self.M], self.field_elems(z))\n",
    "        else:\n",
    "            res = np.empty(len(z), dtype=float)\n",
    "            for i, x in enumerate(z):\n",
    "                res[i] = np.dot(xi[:self.M], self.field_elems(x))\n",
    "            return res\n",
    "\n",
    "    @staticmethod\n",
    "    @jit\n",
    "    def rho(x, y):\n",
    "        \"\"\"\n",
    "        correlation function\n",
    "        \"\"\"\n",
    "        return np.exp(-(np.linalg.norm(x - y)/0.2)**2)\n",
    "\n",
    "    @jit\n",
    "    def C(self, z):\n",
    "        \"\"\"\n",
    "        vector C\n",
    "        \"\"\"\n",
    "        res = np.empty(len(self.pnts), dtype=float)\n",
    "        for i, p in enumerate(self.pnts):\n",
    "            res[i] = self.corr_func(z, p)\n",
    "\n",
    "        return res\n",
    "\n",
    "    def Cmat(self):\n",
    "        \"\"\"\n",
    "        Matrix from C\n",
    "        \"\"\"\n",
    "        return np.array([self.C(p) for p in self.pnts])\n",
    "\n",
    "    @jit\n",
    "    def field_elems(self, z):\n",
    "        \"\"\"\n",
    "        Returns terms separately\n",
    "        Hash for values used\n",
    "        You can specify a different xi and reuse this vals\n",
    "        \"\"\"\n",
    "        str_z = str(z)\n",
    "        if str_z in self.stored_vals:\n",
    "            return self.stored_vals[str_z]\n",
    "\n",
    "        C_z = self.C(z)\n",
    "        vals = np.empty(self.M)\n",
    "        for i, (l, theta) in enumerate(zip(self.l[:self.M], self.theta[:self.M])):\n",
    "            vals[i] = np.dot(theta, C_z)/np.sqrt(l)\n",
    "\n",
    "        self.stored_vals[str_z] = vals\n",
    "\n",
    "        return vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "M = 11\n",
    "KL = EOLE(M=M)\n",
    "Nx = 50\n",
    "X = Gen_uniform_pnts(range_O=[-1, 1], num_1axis=Nx)\n",
    "def plot_eigvsls_n(n):\n",
    "    n -= 1\n",
    "    xi = np.eye(M)[n]\n",
    "    KL.xi = xi\n",
    "    fld = KL(X).reshape(Nx, Nx)\n",
    "    plt.figure(figsize=(7, 7))\n",
    "    plt.imshow(fld.reshape(Nx, Nx), extent=(-1,1,-1,1), vmin=-.35, vmax=0.35);\n",
    "    plt.title(r'$\\phi_{}(x,y)$'.format(n))\n",
    "    plt.colorbar();\n",
    "    \n",
    "controls = {r'n' : widgets.IntSlider(min=1,max=M,step=1,value=1, continuous_update=True,description=r'$n$')}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "interact(plot_eigvsls_n, **controls);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "You can manually mix the the eigenfunctions with the selected weights.\n",
    "This is exactly what happens when we select the realization of a random vector $\\xi=\\{\\xi_1,\\,\\xi_2,\\,\\ldots,\\,\\xi_n\\}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "KL = EOLE(M=4)\n",
    "Nx = 50\n",
    "X = Gen_uniform_pnts(range_O=[-1, 1], num_1axis=Nx)\n",
    "\n",
    "\n",
    "def plot_eigvals(xi1, xi2, xi3, xi4):\n",
    "    KL.xi = np.array([xi1, xi2, xi3, xi4])\n",
    "    fld = KL(X).reshape(Nx, Nx)\n",
    "    plt.figure(figsize=(7, 7))\n",
    "    plt.imshow(fld.reshape(Nx, Nx), extent=(-1,\n",
    "                                            1, -1, 1), vmin=-.35, vmax=0.35)\n",
    "    plt.title('mixture of the first 4 eigenfunctions')\n",
    "    plt.colorbar()\n",
    "\n",
    "\n",
    "vals = [1.0, 0.4, -0.6, 0.5]\n",
    "controls = {r'xi{}'.format(i+1): widgets.FloatSlider(min=-1, max=1, step=0.1,\n",
    "                                                     value=vals[i], continuous_update=True, description=r'$\\xi_{}$'.format(i)) for i in range(4)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "interact(plot_eigvals, **controls);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let us see the dependence of the eigenvalues in the considered 2D case on the correlation length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def gen_corr_func(sigma):\n",
    "    @jit\n",
    "    def rho(x, y):\n",
    "        \"\"\"\n",
    "        correlation function\n",
    "        \"\"\"\n",
    "        return np.exp(-((np.linalg.norm(x - y)/sigma)**2))\n",
    "    return rho\n",
    "\n",
    "\n",
    "M = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "for sigma in [0.01, 0.05, 0.1, 0.2, 0.3, 0.5]:\n",
    "    rho = gen_corr_func(sigma)\n",
    "    KL = EOLE(corr_func=rho)\n",
    "    plt.plot(KL.l[:M], label=r'$\\ell={}$'.format(sigma))\n",
    "plt.legend(loc='upper right')\n",
    "plt.xlabel('$n$')\n",
    "plt.ylabel('$\\lambda_n$');\n",
    "plt.title(r'Dependence of the eigenvalues on the correlation length $\\ell$');\n",
    "#plt.yscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
