{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Lecture 6: Learning hyperparameters for GP. Active learning and Bayesian optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## How to explore the function of interest via GP process?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Setting\n",
    "* Suppose that we have $n$ evaluations of our function of interest $y = f(\\mathbf{x}) + \\varepsilon$, $ \\varepsilon\\sim\\mathcal N(0,\\sigma^2)$:\n",
    "\n",
    "$$\n",
    "\\mathcal D_{n} = \\{(\\mathbf{x}_i,y_i),\\; i = 1,\\ldots,n\\}.\n",
    "$$\n",
    "* And we want to know which points $\\mathbf{x}_{n+1},\\mathbf{x}_{n+2},\\ldots$ we should sample and evaluate the function in order to find maximum/minimum of $f(\\mathbf{x})$ or reduce an approximation error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Bayesian optimization (general algorithm)\n",
    "1. **for** $t=n+1,\\,n+2,\\ldots$\n",
    "2. Find $\\mathbf{x}_t$ by combining attributes of the posterior distribution in a **utility function** $u(x \\mid \\mathcal D_{t-1})$ and maximizing: $x_t = \\text{argmax}_\\mathbf{x} u(\\mathbf x\\mid\\mathcal D_{t-1})$.\n",
    "3. Sample the function $y_t = f(\\mathbf{x}_t) + \\varepsilon_t$.\n",
    "4. Augment the data $\\mathcal D_t = \\{D_{t-1},(\\mathbf{x}_t,y_t)\\}$ and update the GP.\n",
    "5. **end for**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Exploration-exploitation trade-off\n",
    "\n",
    "* In order to find the maximum, we should choose the next input $\\mathbf{x}$ where the mean $\\mu(\\mathbf{x})$ is high (exploitation) and the variance $\\sigma(\\mathbf{x})$ is high (exploration) as well.\n",
    "* This trade-off is accounted in the **acquisition** (or **utility**) function. The simplest utility function is $\\mu(\\mathbf{x})+\\alpha\\cdot\\sigma(\\mathbf{x})$. \n",
    "* If we need to explore (not find the max or min) the function of interest, then the utility function is just a value of standard deviation of GP $u(\\mathbf x\\mid\\mathcal D_{t-1}) = \\sigma(\\mathbf{x})$ \n",
    "* Popular utility functions: **probability of improvement**, **expected improvement**, **GP-UCB**, **Thompson sampling**.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* In the example below, we explore the function of interest \n",
    "$$\n",
    "f(x)=\\frac12\\cos(2x)+\\sin(0.9x)\n",
    "$$\n",
    "with the use of GPR for two sampling approaches: random and adaptive with maximization of s.d. as a utility function. Also, approximation error is considered and compared.\n",
    "\n",
    "* We can change 4 parameters: $\\ell$ is the correlation length, $N$ and $n$ are the number of training and test points, correspondingly, $\\sigma$ is the amount of noise at each point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import GPy\n",
    "from ipywidgets import interactive, interact, widgets\n",
    "import scipy.spatial as SP\n",
    "\n",
    "from scipy.stats import norm\n",
    "\n",
    "\n",
    "def GP(length_scale, n, N, sigma, mode):\n",
    "    # mode: if 0 -- active learning, if 1 -- random selection\n",
    "    np.random.seed(42)\n",
    "\n",
    "    Xtest = np.linspace(-5, 5, n)[:, None]\n",
    "\n",
    "    def f(x): return 0.5*np.cos(2*x.flatten())+np.sin(0.9*x.flatten())\n",
    "\n",
    "    def kernel(a, b):\n",
    "        return np.exp(-.5 * SP.distance.cdist(a, b, 'sqeuclidean')/(length_scale**2))\n",
    "\n",
    "    def fitting(X, num_of_points):\n",
    "        y  = f(X) + sigma*np.random.randn(num_of_points)\n",
    "        K  = kernel(X, X)\n",
    "        L  = np.linalg.cholesky(K + sigma*np.eye(num_of_points))\n",
    "        Lk = np.linalg.solve(L, kernel(X, Xtest))\n",
    "        mu = np.dot(Lk.T, np.linalg.solve(L, y))\n",
    "        K_ = kernel(Xtest, Xtest)\n",
    "        s2 = np.diag(K_) - np.sum(Lk**2, axis=0)\n",
    "        s  = np.sqrt(s2)\n",
    "        return y, mu, s\n",
    "\n",
    "    err_act = []\n",
    "    err_rand = []\n",
    "\n",
    "    for t in range(1, N+1):\n",
    "        mu_act = 0\n",
    "        if t == 1:\n",
    "            X_act = np.random.uniform(-5, 5, size=(1, 1))\n",
    "            _, _, s_act = fitting(X_act, t)\n",
    "            X_act = np.append(\n",
    "                X_act, Xtest[np.argmax(np.array(s_act))]).reshape(t+1, 1)\n",
    "        else:\n",
    "            y_act, mu_act, s_act = fitting(X_act, t)\n",
    "            X_act = np.append(\n",
    "                X_act, Xtest[np.argmax(np.array(s_act))]).reshape(t+1, 1)\n",
    "        X_act = X_act[:N]\n",
    "        err_act.append(np.linalg.norm(f(Xtest)-mu_act, ord=np.inf) /\n",
    "                       np.linalg.norm(f(Xtest), ord=np.inf))\n",
    "\n",
    "    X_rand = np.random.uniform(-5, 5, size=(N, 1))\n",
    "    y_rand, mu_rand, s_rand = fitting(X_rand, N)\n",
    "    for t in range(1, N+1):\n",
    "        mu_rand = 0\n",
    "        X_rand_t = np.copy(X_rand[:t])\n",
    "        _, mu_rand, _ = fitting(X_rand_t, t)\n",
    "        err_rand.append(np.linalg.norm(f(Xtest)-mu_rand,\n",
    "                                       ord=np.inf)/np.linalg.norm(f(Xtest), ord=np.inf))\n",
    "\n",
    "    if mode == 'Adaptive sampling':\n",
    "        y = y_act\n",
    "        X = X_act\n",
    "        mu = mu_act\n",
    "        s = s_act\n",
    "    else:\n",
    "        y = y_rand\n",
    "        X = X_rand\n",
    "        mu = mu_rand\n",
    "        s = s_rand\n",
    "\n",
    "    plt.figure(1, figsize=(9, 7))\n",
    "    plt.clf()\n",
    "    plt.plot(X, y, 'r+', ms=18, label=\"Training points\")\n",
    "    plt.plot(Xtest, f(Xtest), 'b-', label=\"Function\")\n",
    "    plt.fill_between(Xtest.flat, mu-s, mu+s, color=\"#dddddd\",\n",
    "                     label=\"Confidence interval\")\n",
    "    plt.plot(Xtest, mu, 'r--', lw=2, label=\"Approximation\")\n",
    "    plt.fill_between(Xtest.flat, s-3., -3*np.ones(len(Xtest)),\n",
    "                     color=\"green\", alpha=0.1)\n",
    "    plt.plot(Xtest.flat, s-3., 'g-', lw=2, label=\"Acquisition function\")\n",
    "    plt.title(\"Error (inf. norm) = \" + str(round(np.linalg.norm(f(Xtest)-mu, ord=np.inf) /\n",
    "                                                 np.linalg.norm(f(Xtest), ord=np.inf), 4)))  # (r'Mean prediction plus-minus one s.d.')\n",
    "    plt.xlabel('$x$', fontsize=16)\n",
    "    plt.ylabel('$f(x)$', fontsize=16)\n",
    "    plt.axis([-5, 5, -3, 3])\n",
    "    plt.legend()\n",
    "    #print(\"Error (inf. norm) = \", np.linalg.norm(f(Xtest)-mu, ord=np.inf)/np.linalg.norm(f(Xtest), ord=np.inf))\n",
    "\n",
    "    plt.figure(2, figsize=(8, 6))\n",
    "    plt.clf()\n",
    "    plt.plot(range(1, N+1), err_rand, '-o', label=\"Random Choice\")\n",
    "    plt.plot(range(1, N+1), err_act, '-o', label=\"Active Learning\")\n",
    "    plt.xlabel('$N$ number of training points', fontsize=12)\n",
    "    plt.ylabel('$\\epsilon$ approxiamtion error', fontsize=12)\n",
    "    plt.axis([1, N+0.5, 0.001, 1.1])\n",
    "    plt.yscale('log')\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "controls = {r'sigma': widgets.FloatSlider(min=5e-4, max=5e-1, step=1e-3, value=1e-3, continuous_update=True, description=r'$\\sigma$'),\n",
    "            r'length_scale': widgets.FloatSlider(min=0.1, max=2.0, step=0.05, value=0.7, continuous_update=True, description=r'$\\ell$'),\n",
    "            r'N': widgets.IntSlider(min=1, max=50, step=1, value=5, continuous_update=True, description=r'$N$'),\n",
    "            r'n': widgets.IntSlider(min=1, max=100, step=1, value=50, continuous_update=True, description=r'$n$'),\n",
    "            r'mode': widgets.RadioButtons(options=['Adaptive sampling', 'Random sampling'], value='Adaptive sampling', description='Sampling:', disabled=False)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9afae1a7a5664ff2a9a52bf7eee72c96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=0.7, description='$\\\\ell$', max=2.0, min=0.1, step=0.05), IntSlider(vaâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "interact(GP, **controls);  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## How to learn hyperparameters of GP model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Setting\n",
    "* We need to determine parameters of the GP $f(\\mathbf{x})\\sim\\text{GP}(m(\\mathbf{x}),k(\\mathbf{x},\\mathbf{x}'))$.\n",
    "* Let us assume that our model is parameterized as follows:\n",
    "$$\n",
    "\\text{Mean } m(\\mathbf{x})=a\\mathbf{x}^2+b\\mathbf{x}+c, \\text{and}\\\\\n",
    "\\text{Covariance } k(\\mathbf{x}_p,\\mathbf{x}_q) = \\sigma_y^2\\exp{\\left(âˆ’\\frac{\\left|\\mathbf{x}_pâˆ’\\mathbf{x}_q\\right|^2}{2\\ell^2}\\right)} + \\sigma_n^2\\delta_{pq},\n",
    "$$ \n",
    "where hyperparameters are $\\mathbf{\\theta} = \\{a,b,c,\\sigma_y,\\sigma_n,\\ell\\}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* We want to be able to make inferences about all of the hyperparameters having the data $\\mathcal{D}_{n} = {(\\mathbf{x}_i,y_i),i = 1,\\ldots,n}$.\n",
    "* To do this we calculate the probability of the data given the hyperparameters. \n",
    "Denote $\\mathbf \\mu=\\{m(x_i)\\}$, $\\Sigma=\\{k(x_i,x_j)\\}$.\n",
    "Since by assumption the distribution of the data is Gaussian:\n",
    "$$\n",
    "L = \\log{p(\\mathbf{y}\\mid\\mathbf{x},\\mathbf{\\theta})} = -\\frac12\\log\\left|\\Sigma\\right|-\\frac{1}{2}(\\mathbf{y}-\\mathbf{\\mu})^T\\Sigma^{âˆ’1}(\\mathbf{y}âˆ’\\mathbf{\\mu})âˆ’ \\frac n2\\log(2\\pi),\n",
    "$$ we will call this expression as **log marginal likelihood**.\n",
    "* We can now find the values of the hyperparameters which optimizes the marginal likelihood:\n",
    "$$\n",
    "\\frac{\\partial{L}}{\\partial{\\theta_m}} = -(\\mathbf{y}-\\mathbf{\\mu})^T\\Sigma^{âˆ’1}\\frac{\\partial m}{\\partial\\theta_m},\\\\\n",
    "\\frac{\\partial{L}}{\\partial{\\theta_k}} = \\frac{1}{2}\\text{tr}\\left(\\Sigma^{âˆ’1}\\frac{\\partial \\Sigma}{\\partial\\theta_k}\\right) + \\frac{1}{2}(\\mathbf{y}-\\mathbf{\\mu})^T\\frac{\\partial \\Sigma}{\\partial\\theta_k}\\Sigma^{-1}(\\mathbf{y}-\\mathbf{\\mu})\\frac{\\partial \\Sigma}{\\partial\\theta_k}\n",
    "$$\n",
    "where $\\theta_m$ and $\\theta_k$ are used to indicate hyperparameters of the mean and covariance functions respectively.\n",
    "* This problem can be easily solved by numerical optimization methods such as L-BFGS.\n",
    "* **Note:** there can be multiple optima of the marginal likelihood (each locally optimal $\\theta$ corresponds to different interpretation of the data)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In the example below, we will see the result of finding GP model parameters with the use of maximum likelihood. \n",
    "As a function that we approximate, we take \n",
    "$$\n",
    "f(x)=\\frac{1}{2}\\cos(2x)+\\sin(0.9x)\n",
    "$$\n",
    "with the addition of noise $\\sigma = 0.1$.\n",
    "\n",
    "In manual mode one can try to match the parameters (```var``` and $\\ell$ are parameter of the Gaussian kernel, $\\sigma$ is noise value) to reduce the error.  In optimized mode they are calculated automatically -- we will find two different optima by setting different inital values for hyperparameters.\n",
    "We use Python package ```GPy``` for all internal optimization, information about its work is shown in the table below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def f(x): return 0.5*np.cos(2*x)+np.sin(0.9*x)\n",
    "def GP_tune(variance, l, s, mode):\n",
    "    np.random.seed(4)\n",
    "    \n",
    "    X = np.linspace(-5, 5, 20)[:, None]\n",
    "    Xtest = np.linspace(-5, 5, 100)[:, None]\n",
    "    Y = f(X) + np.random.normal(loc=0.0, scale=0.1, size=(20, 1))\n",
    "    \n",
    "    \n",
    "    k = GPy.kern.RBF(input_dim=1, variance=variance, lengthscale=l)\n",
    "    m = GPy.models.GPRegression(X, Y, k)\n",
    "    m.Gaussian_noise = s\n",
    "    if mode == 'Optimize 1':\n",
    "        m.kern.variance = 1.\n",
    "        m.Gaussian_noise = 1.5\n",
    "        m.kern.lengthscale = 1.\n",
    "        m.optimize('lbfgs')\n",
    "    if mode == 'Optimize 2':\n",
    "        m.kern.variance = 1.\n",
    "        m.Gaussian_noise = 0.1\n",
    "        m.kern.lengthscale = 0.1\n",
    "        m.optimize()  \n",
    "    figure, axes = plt.subplots(1,2, figsize=(16,10), tight_layout=True)\n",
    "    m.plot(ax=axes[0])\n",
    "    axes[0].set_title('Error ='+str(round(np.linalg.norm(f(Xtest)-m.predict(Xtest)[0], ord=np.inf)/np.linalg.norm(f(Xtest), ord=np.inf),4)))\n",
    "    print(m)\n",
    "\n",
    "    n = 50\n",
    "\n",
    "    kern = GPy.kern.RBF(1)\n",
    "    model = GPy.models.GPRegression(X, Y, kern)\n",
    "    model.kern.variance = 1.\n",
    "\n",
    "    L = np.zeros((n,n))\n",
    "    i = 0\n",
    "    for gaussian_noise in np.linspace(1e-3, 0.07, n)[:, None]:\n",
    "        j = 0\n",
    "        model.Gaussian_noise = gaussian_noise\n",
    "        for length in np.linspace(0.1, 3, n)[:, None]:\n",
    "            model.kern.lengthscale = length\n",
    "            L[i,j] = model.log_likelihood()\n",
    "            j += 1\n",
    "        i += 1 \n",
    "    x_a = np.linspace(0.1, 3, n)\n",
    "    y_a = np.linspace(1e-3, 0.07, n)\n",
    "    X_a, Y_a = np.meshgrid(x_a, y_a)\n",
    "    plt.ylabel('$\\sigma_n$', fontsize = 15)\n",
    "    plt.xlabel('$\\ell$', fontsize = 15)\n",
    "    axes[1].contour(X_a, Y_a, L, 200,cmap='coolwarm')\n",
    "    axes[1].set_title('Value of log-likelihood from $\\sigma_n$ and $\\ell$')\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "w_variance = widgets.FloatSlider(min=1e-3, max=4., step=1e-2, value=1., continuous_update=True, description=r'var')\n",
    "w_l = widgets.FloatSlider(min=1e-3, max=4., step=1e-2, value=1., continuous_update=True, description=r'$\\ell$')\n",
    "w_s = widgets.FloatSlider(min=1e-3, max=1e-1, step=1e-2, value=1., continuous_update=True, description=r'Ïƒ')\n",
    "w_mode = widgets.RadioButtons(options=['Optimize 1','Optimize 2', 'Manual'], value='Manual', description='Tuning:', disabled=False)\n",
    "def update_mode(*args):\n",
    "    for w in [w_variance, w_l, w_s]:\n",
    "        w.disabled = w_mode.value[0] == 'O'\n",
    "\n",
    "w_mode.observe(update_mode, 'value')    \n",
    "controls = {'variance': w_variance,\n",
    "            'l': w_l,\n",
    "            's': w_s,\n",
    "            'mode': w_mode}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06ddb88a293a4b4292a3620be24f5eb9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=1.0, description='var', max=4.0, min=0.001, step=0.01), FloatSlider(vaâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " C:\\Users\\m.panov\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\matplotlib\\figure.py:2267: UserWarning:This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x720 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "interact(GP_tune, **controls);"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
