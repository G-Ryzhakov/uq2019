{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Lecture 6: Learning hyperparameters for GP. Active learning and Bayesian optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## How to learn hyperparameters of GP model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Setting\n",
    "* We need to determine parameters of the GP $f(\\mathbf{x})\\sim\\text{GP}(m(\\mathbf{x}),k(\\mathbf{x},\\mathbf{x}'))$.\n",
    "* Let us assume that our model is parameterized as follows:\n",
    "$$\n",
    "\\text{Mean } m(\\mathbf{x})=a\\mathbf{x}^2+b\\mathbf{x}+c, \\text{and}\\\\\n",
    "\\text{Covariance } k(\\mathbf{x}_p,\\mathbf{x}_q) = \\sigma_y^2\\exp{\\left(−\\frac{\\left|\\mathbf{x}_p−\\mathbf{x}_q\\right|^2}{2\\ell^2}\\right)} + \\sigma_n^2\\delta_{pq},\n",
    "$$ \n",
    "where hyperparameters are $\\mathbf{\\theta} = \\{a,b,c,\\sigma_y,\\sigma_n,\\ell\\}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* We want to be able to make inferences about all of the hyperparameters having the data $\\mathcal{D}_{n} = {(\\mathbf{x}_i,y_i),i = 1,\\ldots,n}$.\n",
    "* To do this we calculate the probability of the data given the hyperparameters. \n",
    "Denote $\\mathbf \\mu=\\{m(x_i)\\}$, $\\Sigma=\\{k(x_i,x_j)\\}$.\n",
    "Since by assumption the distribution of the data is Gaussian:\n",
    "$$\n",
    "L = \\log{p(\\mathbf{y}\\mid\\mathbf{x},\\mathbf{\\theta})} = -\\frac12\\log\\left|\\Sigma\\right|-\\frac{1}{2}(\\mathbf{y}-\\mathbf{\\mu})^T\\Sigma^{−1}(\\mathbf{y}−\\mathbf{\\mu})− \\frac n2\\log(2\\pi),\n",
    "$$ we will call this expression as **log marginal likelihood**.\n",
    "* We can now find the values of the hyperparameters which optimizes the marginal likelihood:\n",
    "$$\n",
    "\\frac{\\partial{L}}{\\partial{\\theta_m}} = -(\\mathbf{y}-\\mathbf{\\mu})^T\\Sigma^{−1}\\frac{\\partial m}{\\partial\\theta_m},\\\\\n",
    "\\frac{\\partial{L}}{\\partial{\\theta_k}} = \\frac{1}{2}\\text{tr}\\left(\\Sigma^{−1}\\frac{\\partial \\Sigma}{\\partial\\theta_k}\\right) + \\frac{1}{2}(\\mathbf{y}-\\mathbf{\\mu})^T\\frac{\\partial \\Sigma}{\\partial\\theta_k}\\Sigma^{-1}(\\mathbf{y}-\\mathbf{\\mu})\\frac{\\partial \\Sigma}{\\partial\\theta_k}\n",
    "$$\n",
    "where $\\theta_m$ and $\\theta_k$ are used to indicate hyperparameters of the mean and covariance functions respectively.\n",
    "* This problem can be easily solved by numerical optimization methods such as L-BFGS.\n",
    "* **Note:** there can be multiple optima of the marginal likelihood (each locally optimal $\\theta$ corresponds to different interpretation of the data)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In the example below, we will see the result of finding GP model parameters with the use of maximum likelihood. \n",
    "As a function that we approximate, we take \n",
    "$$\n",
    "f(x)=\\frac{1}{2}\\cos(2x)+\\sin(0.9x)\n",
    "$$\n",
    "with the addition of noise $\\sigma = 0.1$.\n",
    "\n",
    "In manual mode one can try to match the parameters (```var``` and $\\ell$ are parameter of the Gaussian kernel, $\\sigma$ is noise value) to reduce the error.  In optimized mode they are calculated automatically -- we will find two different optima by setting different inital values for hyperparameters.\n",
    "We use Python package ```GPy``` for all internal optimization, information about its work is shown in the table below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import GPy\n",
    "from ipywidgets import interactive, interact, widgets\n",
    "import scipy.spatial as SP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def f(x): return 0.5*np.cos(2*x)+np.sin(0.9*x)\n",
    "def GP_tune(variance, l, s, mode):\n",
    "    np.random.seed(4)\n",
    "    \n",
    "    X = np.linspace(-5, 5, 20)[:, None]\n",
    "    Xtest = np.linspace(-5, 5, 100)[:, None]\n",
    "    Y = f(X) + np.random.normal(loc=0.0, scale=0.1, size=(20, 1))\n",
    "    \n",
    "    \n",
    "    k = GPy.kern.RBF(input_dim=1, variance=variance, lengthscale=l)\n",
    "    m = GPy.models.GPRegression(X, Y, k)\n",
    "    m.Gaussian_noise = s\n",
    "    if mode == 'Optimize 1':\n",
    "        m.kern.variance = 1.\n",
    "        m.Gaussian_noise = 1.5\n",
    "        m.kern.lengthscale = 1.\n",
    "        m.optimize('lbfgs')\n",
    "    if mode == 'Optimize 2':\n",
    "        m.kern.variance = 1.\n",
    "        m.Gaussian_noise = 0.1\n",
    "        m.kern.lengthscale = 0.1\n",
    "        m.optimize()  \n",
    "    figure, axes = plt.subplots(1,2, figsize=(16,10), tight_layout=True)\n",
    "    m.plot(ax=axes[0])\n",
    "    axes[0].set_title('Error ='+str(round(np.linalg.norm(f(Xtest)-m.predict(Xtest)[0], ord=np.inf)/np.linalg.norm(f(Xtest), ord=np.inf),4)))\n",
    "    print(m)\n",
    "\n",
    "    n = 50\n",
    "\n",
    "    kern = GPy.kern.RBF(1)\n",
    "    model = GPy.models.GPRegression(X, Y, kern)\n",
    "    model.kern.variance = 1.\n",
    "\n",
    "    L = np.zeros((n,n))\n",
    "    i = 0\n",
    "    for gaussian_noise in np.linspace(1e-3, 0.07, n)[:, None]:\n",
    "        j = 0\n",
    "        model.Gaussian_noise = gaussian_noise\n",
    "        for length in np.linspace(0.1, 3, n)[:, None]:\n",
    "            model.kern.lengthscale = length\n",
    "            L[i,j] = model.log_likelihood()\n",
    "            j += 1\n",
    "        i += 1 \n",
    "    x_a = np.linspace(0.1, 3, n)\n",
    "    y_a = np.linspace(1e-3, 0.07, n)\n",
    "    X_a, Y_a = np.meshgrid(x_a, y_a)\n",
    "    plt.ylabel('$\\sigma_n$', fontsize = 15)\n",
    "    plt.xlabel('$\\ell$', fontsize = 15)\n",
    "    axes[1].contour(X_a, Y_a, L, 200,cmap='coolwarm')\n",
    "    axes[1].set_title('Value of log-likelihood from $\\sigma_n$ and $\\ell$')\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "w_variance = widgets.FloatSlider(min=1e-3, max=4., step=1e-2, value=1., continuous_update=True, description=r'var')\n",
    "w_l = widgets.FloatSlider(min=1e-3, max=4., step=1e-2, value=1., continuous_update=True, description=r'$\\ell$')\n",
    "w_s = widgets.FloatSlider(min=1e-3, max=1e-1, step=1e-2, value=1., continuous_update=True, description=r'σ')\n",
    "w_mode = widgets.RadioButtons(options=['Optimize 1','Optimize 2', 'Manual'], value='Manual', description='Tuning:', disabled=False)\n",
    "def update_mode(*args):\n",
    "    for w in [w_variance, w_l, w_s]:\n",
    "        w.disabled = w_mode.value[0] == 'O'\n",
    "\n",
    "w_mode.observe(update_mode, 'value')    \n",
    "controls = {'variance': w_variance,\n",
    "            'l': w_l,\n",
    "            's': w_s,\n",
    "            'mode': w_mode}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "interact(GP_tune, **controls);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Building GPR model\n",
    "\n",
    "Lets fit GPR model for function $f(x) = − \\cos(\\pi x) + \\sin(4\\pi x)$ in $[0, 1]$,\n",
    "with noise $y(x) = f(x) + \\epsilon$, $\\epsilon \\sim \\mathcal{N}(0, 0.1)$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "N = 10\n",
    "X = np.linspace(0.05, 0.95, N).reshape(-1, 1)\n",
    "Y = -np.cos(np.pi * X) + np.sin(4 * np.pi * X) + \\\n",
    "    np.random.normal(loc=0.0, scale=0.1, size=(N, 1))\n",
    "plt.figure(figsize=(5, 3))\n",
    "plt.plot(X, Y, '.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### 1. Define covariance function\n",
    "\n",
    "The most popular kernel - RBF kernel - has 2 parameters: `variance` and `lengthscale`, $k(x, y) = \\sigma^2 \\exp\\left ( -\\dfrac{\\|x - y\\|^2}{2l^2}\\right )$,\n",
    "where `variance` is $\\sigma^2$, and `lengthscale` - $l$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "input_dim = 1\n",
    "variance = 1\n",
    "lengthscale = 0.2\n",
    "kernel = GPy.kern.RBF(input_dim, variance=variance,\n",
    "                      lengthscale=lengthscale)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### 2. Create GPR model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "model = GPy.models.GPRegression(X, Y, kernel)\n",
    "print(model)\n",
    "model.plot(figsize=(5, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Parameters of the covariance function\n",
    "\n",
    "Values of parameters of covariance function can be set like:  `k.lengthscale = 0.1`.\n",
    "\n",
    "Let's change the value of `lengthscale` parameter and see how it changes the covariance function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = GPy.kern.RBF(1)\n",
    "theta = np.asarray([0.2, 0.5, 1, 2, 4, 10])\n",
    "figure, axes = plt.subplots(2, 3, figsize=(8, 4))\n",
    "for t, ax in zip(theta, axes.ravel()):\n",
    "    k.lengthscale = t\n",
    "    k.plot(ax=ax)\n",
    "    ax.set_ylim([0, 1])\n",
    "    ax.set_xlim([-4, 4])\n",
    "    ax.legend([t])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's try to change parameters to obtain more accurate model (we fix noise variance to some more correct value)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "kernel = GPy.kern.RBF(1, lengthscale=0.1)\n",
    "model = GPy.models.GPRegression(X, Y, kernel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "model.Gaussian_noise.variance.fix(0.01)\n",
    "print(model)\n",
    "model.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Tuning parameters of the covariance function\n",
    "\n",
    "The parameters are tuned by maximizing likelihood. To do it just use `optimize()` method of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "model = GPy.models.GPRegression(X, Y, kernel)\n",
    "model.optimize()\n",
    "print(model)\n",
    "model.plot(figsize=(5, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Noise variance\n",
    "\n",
    "Noise variance acts like a regularization in GP models. Larger values of noise variance lead to more smooth model.  \n",
    "Let's check it: try to change noise variance to some large value, to some small value and see the results.\n",
    "\n",
    "Noise variance accessed like this: `model.Gaussian_noise.variance = 1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "model.Gaussian_noise.variance = 0.01\n",
    "model.plot(figsize=(5, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Now, let's generate more noisy data and try to fit model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "N = 40\n",
    "X = np.linspace(0.05, 0.95, N).reshape(-1, 1)\n",
    "Y = -np.cos(np.pi * X) + np.sin(4 * np.pi * X) + \\\n",
    "    np.random.normal(loc=0.0, scale=0.5, size=(N, 1))\n",
    "\n",
    "kernel = GPy.kern.RBF(1)\n",
    "model = GPy.models.GPRegression(X, Y, kernel)\n",
    "model.optimize()\n",
    "print(model)\n",
    "model.plot(figsize=(5, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Now, let's fix noise variance to some small value and fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "kernel = GPy.kern.RBF(1)\n",
    "model = GPy.models.GPRegression(X, Y, kernel)\n",
    "model.Gaussian_noise.variance.fix(0.01)\n",
    "model.optimize()\n",
    "model.plot(figsize=(5, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## How to explore the function of interest via GP process?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Setting\n",
    "* Suppose that we have $n$ evaluations of our function of interest $y = f(\\mathbf{x}) + \\varepsilon$, $ \\varepsilon\\sim\\mathcal N(0,\\sigma^2)$:\n",
    "\n",
    "$$\n",
    "\\mathcal D_{n} = \\{(\\mathbf{x}_i,y_i),\\; i = 1,\\ldots,n\\}.\n",
    "$$\n",
    "* And we want to know which points $\\mathbf{x}_{n+1},\\mathbf{x}_{n+2},\\ldots$ we should sample and evaluate the function in order to find maximum/minimum of $f(\\mathbf{x})$ or reduce an approximation error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Active Learning (general algorithm)\n",
    "1. **for** $t=n+1,\\,n+2,\\ldots$\n",
    "2. Find $\\mathbf{x}_t$ by combining attributes of the posterior distribution in a **utility function** $u(x \\mid \\mathcal D_{t-1})$ and maximizing: $x_t = \\text{argmax}_\\mathbf{x} u(\\mathbf x\\mid\\mathcal D_{t-1})$.\n",
    "3. Sample the function $y_t = f(\\mathbf{x}_t) + \\varepsilon_t$.\n",
    "4. Augment the data $\\mathcal D_t = \\{D_{t-1},(\\mathbf{x}_t,y_t)\\}$ and update the GP.\n",
    "5. **end for**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* In the example below, we explore the function of interest \n",
    "$$\n",
    "f(x)=\\frac12\\cos(2x)+\\sin(0.9x)\n",
    "$$\n",
    "with the use of GPR for two sampling approaches: random and adaptive with maximization of an utility function. Also, approximation error is considered and compared.\n",
    "\n",
    "* The utility function is just a value of standard deviation of GP $u(\\mathbf x\\mid\\mathcal D_{t-1}) = \\sigma(\\mathbf{x})$. \n",
    "\n",
    "* We can change 4 parameters: $\\ell$ is the correlation length, $N$ and $n$ are the number of training and test points, correspondingly, $\\sigma$ is the amount of noise at each point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import GPy\n",
    "from ipywidgets import interactive, interact, widgets\n",
    "import scipy.spatial as SP\n",
    "\n",
    "from scipy.stats import norm\n",
    "\n",
    "\n",
    "def GP(length_scale, n, N, sigma, mode):\n",
    "    # mode: if 0 -- active learning, if 1 -- random selection\n",
    "    np.random.seed(42)\n",
    "\n",
    "    Xtest = np.linspace(-5, 5, n)[:, None]\n",
    "\n",
    "    def f(x): return 0.5*np.cos(2*x.flatten())+np.sin(0.9*x.flatten())\n",
    "\n",
    "    def kernel(a, b):\n",
    "        return np.exp(-.5 * SP.distance.cdist(a, b, 'sqeuclidean')/(length_scale**2))\n",
    "\n",
    "    def fitting(X, num_of_points):\n",
    "        y  = f(X) + sigma*np.random.randn(num_of_points)\n",
    "        K  = kernel(X, X)\n",
    "        L  = np.linalg.cholesky(K + sigma*np.eye(num_of_points))\n",
    "        Lk = np.linalg.solve(L, kernel(X, Xtest))\n",
    "        mu = np.dot(Lk.T, np.linalg.solve(L, y))\n",
    "        K_ = kernel(Xtest, Xtest)\n",
    "        s2 = np.diag(K_) - np.sum(Lk**2, axis=0)\n",
    "        s  = np.sqrt(s2)\n",
    "        return y, mu, s\n",
    "\n",
    "    err_act = []\n",
    "    err_rand = []\n",
    "\n",
    "    for t in range(1, N+1):\n",
    "        mu_act = 0\n",
    "        if t == 1:\n",
    "            X_act = np.random.uniform(-5, 5, size=(1, 1))\n",
    "            _, _, s_act = fitting(X_act, t)\n",
    "            X_act = np.append(\n",
    "                X_act, Xtest[np.argmax(np.array(s_act))]).reshape(t+1, 1)\n",
    "        else:\n",
    "            y_act, mu_act, s_act = fitting(X_act, t)\n",
    "            X_act = np.append(\n",
    "                X_act, Xtest[np.argmax(np.array(s_act))]).reshape(t+1, 1)\n",
    "        X_act = X_act[:N]\n",
    "        err_act.append(np.linalg.norm(f(Xtest)-mu_act, ord=np.inf) /\n",
    "                       np.linalg.norm(f(Xtest), ord=np.inf))\n",
    "\n",
    "    X_rand = np.random.uniform(-5, 5, size=(N, 1))\n",
    "    y_rand, mu_rand, s_rand = fitting(X_rand, N)\n",
    "    for t in range(1, N+1):\n",
    "        mu_rand = 0\n",
    "        X_rand_t = np.copy(X_rand[:t])\n",
    "        _, mu_rand, _ = fitting(X_rand_t, t)\n",
    "        err_rand.append(np.linalg.norm(f(Xtest)-mu_rand,\n",
    "                                       ord=np.inf)/np.linalg.norm(f(Xtest), ord=np.inf))\n",
    "\n",
    "    if mode == 'Adaptive sampling':\n",
    "        y = y_act\n",
    "        X = X_act\n",
    "        mu = mu_act\n",
    "        s = s_act\n",
    "    else:\n",
    "        y = y_rand\n",
    "        X = X_rand\n",
    "        mu = mu_rand\n",
    "        s = s_rand\n",
    "\n",
    "    plt.figure(1, figsize=(9, 7))\n",
    "    plt.clf()\n",
    "    plt.plot(X, y, 'r+', ms=18, label=\"Training points\")\n",
    "    plt.plot(Xtest, f(Xtest), 'b-', label=\"Function\")\n",
    "    plt.fill_between(Xtest.flat, mu-s, mu+s, color=\"#dddddd\",\n",
    "                     label=\"Confidence interval\")\n",
    "    plt.plot(Xtest, mu, 'r--', lw=2, label=\"Approximation\")\n",
    "    plt.fill_between(Xtest.flat, s-3., -3*np.ones(len(Xtest)),\n",
    "                     color=\"green\", alpha=0.1)\n",
    "    plt.plot(Xtest.flat, s-3., 'g-', lw=2, label=\"Acquisition function\")\n",
    "    plt.title(\"Error (inf. norm) = \" + str(round(np.linalg.norm(f(Xtest)-mu, ord=np.inf) /\n",
    "                                                 np.linalg.norm(f(Xtest), ord=np.inf), 4)))  # (r'Mean prediction plus-minus one s.d.')\n",
    "    plt.xlabel('$x$', fontsize=16)\n",
    "    plt.ylabel('$f(x)$', fontsize=16)\n",
    "    plt.axis([-5, 5, -3, 3])\n",
    "    plt.legend()\n",
    "    #print(\"Error (inf. norm) = \", np.linalg.norm(f(Xtest)-mu, ord=np.inf)/np.linalg.norm(f(Xtest), ord=np.inf))\n",
    "\n",
    "    plt.figure(2, figsize=(8, 6))\n",
    "    plt.clf()\n",
    "    plt.plot(range(1, N+1), err_rand, '-o', label=\"Random Choice\")\n",
    "    plt.plot(range(1, N+1), err_act, '-o', label=\"Active Learning\")\n",
    "    plt.xlabel('$N$ number of training points', fontsize=12)\n",
    "    plt.ylabel('$\\epsilon$ approxiamtion error', fontsize=12)\n",
    "    plt.axis([1, N+0.5, 0.001, 1.1])\n",
    "    plt.yscale('log')\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "controls = {r'sigma': widgets.FloatSlider(min=5e-4, max=5e-1, step=1e-3, value=1e-3, continuous_update=True, description=r'$\\sigma$'),\n",
    "            r'length_scale': widgets.FloatSlider(min=0.1, max=2.0, step=0.05, value=0.7, continuous_update=True, description=r'$\\ell$'),\n",
    "            r'N': widgets.IntSlider(min=1, max=50, step=1, value=5, continuous_update=True, description=r'$N$'),\n",
    "            r'n': widgets.IntSlider(min=1, max=100, step=1, value=50, continuous_update=True, description=r'$n$'),\n",
    "            r'mode': widgets.RadioButtons(options=['Adaptive sampling', 'Random sampling'], value='Adaptive sampling', description='Sampling:', disabled=False)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interact(GP, **controls);  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# When Bayesian optimization?\n",
    "\n",
    "* Optimization of \"heavy\" functions \n",
    "* The target function is a blackbox, typically noisy, while smooth\n",
    "\n",
    "\n",
    "* Construction a regression model using available data\n",
    "* Take into account uncertainty of the regression model\n",
    "* Gaussian process regression is OK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Optimization workflow:\n",
    "1. Construct a regression model $\\hat{f}(x)$ of a function $f(x)$ using the sample $D = \\{(x_i, f(x_i))\\}_{i = 1}^n$\n",
    "2. Select a new point that maximize an acquisition function\n",
    "$$\n",
    "x_{new} = \\arg\\max\\limits_x a(x)\n",
    "$$\n",
    "3. Calculate $f(x_{new})$ at the new point.\n",
    "4. Add the pair $(x_{new}, f(x_{new}))$ to the sample $D$.\n",
    "5. Update the model $\\hat{f}(x)$ and go to step 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Exploration-exploitation trade-off\n",
    "\n",
    "* In order to find the maximum, we should choose the next input $\\mathbf{x}$ where the mean $\\mu(\\mathbf{x})$ is high (exploitation) and the variance $\\sigma(\\mathbf{x})$ is high (exploration) as well.\n",
    "* This trade-off is accounted in the **acquisition** (or **utility**) function.\n",
    "\n",
    "* Popular utility functions: **probability of improvement**, **expected improvement**, **GP-UCB**.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## Examples of the acquisition functions \n",
    "\n",
    "#### Upper confidence bound (UCB) \n",
    "\n",
    "$$\n",
    "UСB(x) = \\hat{f}(x) + \\beta \\hat{\\sigma}(x),\n",
    "$$\n",
    "$\\hat{f}(x), \\hat{\\sigma}(x)$ - mean and standard deviation of the Gaussian process regression model at $x$.\n",
    "\n",
    "#### Expected Improvement (EI) \n",
    "\n",
    "$$\n",
    "EI(x) = \\mathbb{E}_{p(\\hat{f})} \\max(0, f_{min} - \\hat{f}(x)). \n",
    "$$\n",
    "\n",
    "\n",
    "Usually we use logarithm of EI."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"figures/EI_vs_logEI.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 2. Bayesian optimization libraries:\n",
    "\n",
    "| Library        | #commits           | #stars | #last commit |\n",
    "| ------------- | -----:| -----:| -----:|\n",
    "| hyperopt      | 972   | 3798  | 28.05.2019 | \n",
    "| BayesOpt      | 518   | 222   | 28.08.2018 | \n",
    "| GPyOpt        | 485   | 515   | 25.09.2019 |\n",
    "| GPflowOpt     | 426   | 191   | 12.09.2018 | \n",
    "| pyGPGO        | 292   | 158   | 15.06.2019 | \n",
    "\n",
    "More libraries for Matlab (SUMO) and other languages.\n",
    "\n",
    "*Actually it is not hard to write your own library on top of your favorite Gaussian Process Regression library.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Additional libraries\n",
    "\n",
    "We need libraries for\n",
    "* Gaussian process regression **GPy** (see previous seminar)\n",
    "* Gaussian process regression-based Bayesian optimization **GPyOpt**\n",
    "\n",
    "See more use cases of **GPyOpt** at http://nbviewer.jupyter.org/github/SheffieldML/GPyOpt/blob/master/manual/index.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "To install GPyOpt run the following line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "!pip install emcee\n",
    "!pip install GPyOpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import GPy\n",
    "import GPyOpt\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# auxiliary functions\n",
    "import utility\n",
    "% matplotlib inline\n",
    "from IPython.display import clear_output\n",
    "from tqdm import trange\n",
    "\n",
    "# emcee sampler is required to run Entropy search in GPyOpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. One dimensional example\n",
    "\n",
    "We demonstrate concepts using one-dimensional example.\n",
    "\n",
    "Let us consider Bayesian optimization for one-dimensional function **Forrester**:\n",
    "$$\n",
    "f(x) = (6 x - 2)^2 \\sin(12 x - 4).\n",
    "$$\n",
    "\n",
    "The optimization problem is the following:\n",
    "$$\n",
    "f(x) \\rightarrow \\min, x \\in [0, 1].\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# we can load it from GPyOpt library\n",
    "forrester_function = GPyOpt.objective_examples.experiments1d.forrester()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "forrester_function.f(np.array([0.5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "forrester_function.bounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "forrester_function.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Select the region where we search the optimum\n",
    "space = [{'name': 'x', 'type': 'continuous', 'domain': (0, 1)}]\n",
    "design_region = GPyOpt.Design_space(space=space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Select the initial design\n",
    "from numpy.random import seed # fixed seed\n",
    "seed(123456)\n",
    "\n",
    "initial_sample_size = 5\n",
    "initial_design = GPyOpt.experiment_design.initial_design('random', design_region, initial_sample_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "uniform_dense_grid = np.linspace(0, 1, 200).reshape(-1, 1)\n",
    "\n",
    "# plot function: curve and values at the initial design points\n",
    "utility.plot_one_dimensional_function(forrester_function, \n",
    "                                      uniform_dense_grid, \n",
    "                                      initial_design)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## We defined the problem - now we create a machine to solve it\n",
    "\n",
    "1. A black box that evaluates the target function\n",
    "2. What kind of the regression model we need\n",
    "3. How do we optimize the acquisition function\n",
    "4. What kind of the acquisition function we use\n",
    "5. Should we use optimizer in batch or continuous mode?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# The target function\n",
    "objective = GPyOpt.core.task.SingleObjective(forrester_function.f)\n",
    "\n",
    "# Model type\n",
    "gp_model = GPyOpt.models.GPModel(exact_feval=True, optimize_restarts=10, verbose=False) \n",
    "# exact_feval - are evaluations exact?\n",
    "# optimize_restarts - number of restarts at each step\n",
    "# verbose - how verbose we are\n",
    "\n",
    "# Optimizer of the acquisition function, the default is 'LBFGS'\n",
    "aquisition_optimizer = GPyOpt.optimization.AcquisitionOptimizer(design_region)\n",
    "\n",
    "# The acquisition function is expected improvement\n",
    "acquisition_function = GPyOpt.acquisitions.AcquisitionEI(gp_model, design_region, optimizer=aquisition_optimizer)\n",
    "\n",
    "# How we collect the data\n",
    "evaluator = GPyOpt.core.evaluators.Sequential(acquisition_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Now we are ready to construct the machine\n",
    "bayesian_optimizer = GPyOpt.methods.ModularBayesianOptimization(gp_model, design_region, objective, \n",
    "                                                acquisition_function, evaluator, initial_design)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Run the first six iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stopping criteria\n",
    "max_time = None \n",
    "max_number_of_iterations = 5\n",
    "tolerance = 1e-8 # distance between consequitive observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run five iterations\n",
    "for iteration in range(max_number_of_iterations):\n",
    "    bayesian_optimizer.run_optimization(max_iter=1, max_time=max_time, \n",
    "                                        eps=tolerance, verbosity=False) \n",
    "  \n",
    "    bayesian_optimizer.plot_acquisition()\n",
    "    clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Now we run more iterations - 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "bayesian_optimizer = GPyOpt.methods.ModularBayesianOptimization(gp_model, design_region, objective, \n",
    "                                                acquisition_function, evaluator, initial_design)\n",
    "\n",
    "max_number_of_iterations = 25\n",
    "bayesian_optimizer.run_optimization(max_iter=max_number_of_iterations, max_time=max_time, \n",
    "                                    eps=tolerance, verbosity=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Analyze problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "bayesian_optimizer.plot_acquisition()\n",
    "bayesian_optimizer.plot_convergence()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print('Obtained xmin:  %.3f, real xmin:  %.3f (approximate)' % (bayesian_optimizer.x_opt, forrester_function.min))\n",
    "print('Obtained fmin: %.3f, real fmin: %.3f (approximate)' % (bayesian_optimizer.fx_opt, forrester_function.fmin))"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
